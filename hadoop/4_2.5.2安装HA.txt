1.下载,解压
  查看hadoop-2.5.2.tar.gz是32位还是64位
	lib/native file libhadoop.so.1.0.0
  安装zookeeper，配置集群
		conf cp zoo_sample.cfg zoo.cfg
		修改 dataDir=/opt/zookeeper
		添加
		server.1=172.16.129.130:2888:3888
		server.2=172.16.129.131:2888:3888
		server.3=172.16.129.132:2888:3888
		/opt/zookeeper新建myid文件，内容为1，node1为2，node2为3
		添加环境变量 /etc/profile export PATH=$PATH:/usr/local/zookeeper/zookeeper-3.4.7/bin
		source /etc/profile
		service iptables stop
		zkServer.sh start 每台都要启动
		jps 查看java进程
	
2.服务器准备
	NameNode：2台，主备
	DataNode：3台
	ZooKeeper: 3台(大于1的奇数)
	ZooKeeperFailoverController：2台(紧跟NameNode)
	JournalNode：3台(大于1的奇数)
	ResourceManage: 1台(Mapreduce的主)
	DatanodeManage(ApplicationMaster): 3台(紧跟DataNode)
							NN		DN		ZK		ZKFC	JN		RM		DM
	node0: 172.16.129.130	1				1 		1  				1	
	node1: 172.16.129.131	1		1 		1 		1 		1 				1
	node2: 172.16.129.132			1 		1 				1 				1 
	node3: 172.16.129.133			1 						1 				1
	
3.配置
	hdfs-site.xml
		<!-- 服务名 -->
		<property>
			<name>dfs.nameservices</name>
			<value>democluster</value>
		</property>
		<!-- NameNode的名字 -->
		<property>
			<name>dfs.ha.namenodes.democluster</name>
			<value>nn1,nn2</value>
		</property>
		<!-- 两个RPC协议的主机和端口 -->
		<property>
			<name>dfs.namenode.rpc-address.democluster.nn1</name>
			<value>172.16.129.130:8020</value>
		</property>
		<property>
			<name>dfs.namenode.rpc-address.democluster.nn2</name>
			<value>172.16.129.131:8020</value>
		</property>
		<!-- 两个http协议的主机和端口 -->
		<property>
			<name>dfs.namenode.http-address.democluster.nn1</name>
			<value>172.16.129.130:50070</value>
		</property>
		<property>
			<name>dfs.namenode.http-address.democluster.nn2</name>
			<value>172.16.129.131:50070</value>
		</property>
		<!-- JournalNode的主机和端口 -->
		<property>
			<name>dfs.namenode.shared.edits.dir</name>
			<value>qjournal://172.16.129.131:8485;172.16.129.132:8485;172.16.129.133:8485/democluster</value>
		</property>
		<!-- 客户端通过这个类去寻找activity NameNode -->
		<property>
			<name>dfs.client.failover.proxy.provider.democluster</name>
			<value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
		</property>
		<!-- sshfence -->
		<property>
			<name>dfs.ha.fencing.methods</name>
			<value>sshfence</value>
		</property>
		<property>
			<name>dfs.ha.fencing.ssh.private-key-files</name>
			<value>/root/.ssh/id_dsa</value>
		</property>
		<!-- journalnode的工作目录 -->
		<property>
			<name>dfs.journalnode.edits.dir</name>
			<value>/opt/hadoop-2.5.2/journalnode/data</value>
		</property>
		<!-- 开启自动切换 -->
		<property>
			<name>dfs.ha.automatic-failover.enabled</name>
			<value>true</value>
		</property>
		
	core-site.xml
		<!-- NameNode的入口 -->
		<property>
			<name>fs.defaultFS</name>
			<value>hdfs://democluster</value>
		</property>
		<!-- zookeeper的集群 -->
		<property>
			<name>ha.zookeeper.quorum</name>
			<value>172.16.129.130:2181,172.16.129.131:2181,172.16.129.132:2181</value>
		</property>
		<!-- 临时目录，namenode和datanode的工作目录都是以这个为基础的 -->
		<property>
			 <name>hadoop.tmp.dir</name> 
			 <value>/opt/hadoop-2.5.2</value>
		</property>
		
	slaves: 配置datanode的IP地址
		172.16.129.131
		172.16.129.132
		172.16.129.133
	
	etc/hadoop/hadoop-env.sh
		修改 JAVA_HOME
		
		
4.启动
	#node1,2,3上启动journalnode
		sbin ./hadoop-daemon.sh start journalnode
	#在一台namenode(node0)上格式化
		bin ./hdfs namenode -format
	#把元数据从 格式化的机子(node0)上copy到非格式化的机子(node1)
		sbin ./hadoop-daemon.sh start namenode #先启动node0，node0上运行
		bin ./hdfs namenode -bootstrapStandby #node1上运行
	#停止所有服务，包括zookeeper
		sbin ./stop-dfs.sh (node0)
		zkServer.sh stop (node0,1,2)
	#一次性全部启动
		zkServer.sh start (node0,1,2)
		bin ./hdfs zkfc -formatZK (其中一个namenode)
		sbin ./start-dfs.sh
	
	
		
		